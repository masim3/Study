{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c5bf34d",
   "metadata": {},
   "source": [
    "# **KogSys-ML-B Introduction to Machine Learning**\n",
    "## **Ensembles and Evaluation**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf57dd6",
   "metadata": {},
   "source": [
    "To set up a conda environment suitable for this notebook, you can use the following console commands:\n",
    "\n",
    "```bash\n",
    "conda create -y -n ens-eval python=3.13\n",
    "conda activate ens-eval\n",
    "python -m pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "**Note**: Conda can become very hard-drive hungry when you use many environments. Consider regularly deleting environments you no longer need and running the ``conda clean --all`` command to remove no longer needed packages and cached files.\n",
    "\n",
    "You can also install the requirements for this notebook into an existing environment by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8be3c884",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9be3281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.typing import ArrayLike\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee2af83",
   "metadata": {},
   "source": [
    "### **Data Preprocessing**\n",
    "\n",
    "Last time, we worked with a dataset which was already set up to be used with ``scikit-learn``. Today, we will work with a less favorable base and learn to work around it, \"wrangling\" our raw data into a shape we can work with.\n",
    "\n",
    "The dataset we will be working with today is the Spotify tracks dataset, which is available on [Huggingface](https://huggingface.co/datasets/maharshipandya/spotify-tracks-dataset). However, while this dataset is almost already usable, we will consider a modified version to learn some basic data transformations which will be helpful to you on any future Machine Learning tasks.\n",
    "\n",
    "With this notebook, you downloaded four files: ``spotify-1.csv``, ``spotify-2.csv``, ``spotify-3.parquet``, and ``spotify-test.csv``:\n",
    "- ``spotify-1.csv`` and ``spotify-2.csv`` contain the same rows, identified by the column ``\"track_id\"``, but different columns.\n",
    "- ``spotify-3.parquet`` contains additional, complete rows, but is saved in a different file format and some column types don't match.\n",
    "- ``spotify-test.csv`` contains the complete test data. No modifications are needed, but you are not allowed to use this for any purpose during training, only to evaluate the **final** model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d695c42c",
   "metadata": {},
   "source": [
    "#### **Task: Load ``spotify-1.csv`` and ``spotify-2.csv`` and join them _on_ the column ``\"track_id\"``**\n",
    "\n",
    "The resulting DataFrame should have the shape ``(43075, 20)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1bd97ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>artists</th>\n",
       "      <th>album_name</th>\n",
       "      <th>track_name</th>\n",
       "      <th>popularity</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>explicit</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>track_genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67hiQr5yocRHh4fheSEzsC</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.0288</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3690</td>\n",
       "      <td>0.229</td>\n",
       "      <td>Seiko Matsuda</td>\n",
       "      <td>My Prelude</td>\n",
       "      <td>いくつの夜明けを数えたら</td>\n",
       "      <td>27</td>\n",
       "      <td>303626</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>-7.236</td>\n",
       "      <td>1</td>\n",
       "      <td>134.649</td>\n",
       "      <td>4</td>\n",
       "      <td>j-idol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7aUuoq4oMfLxaLa5GVUDHi</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.0837</td>\n",
       "      <td>0.264</td>\n",
       "      <td>KALEO</td>\n",
       "      <td>Way down We Go</td>\n",
       "      <td>Way down We Go</td>\n",
       "      <td>64</td>\n",
       "      <td>219560</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>-5.798</td>\n",
       "      <td>0</td>\n",
       "      <td>81.663</td>\n",
       "      <td>4</td>\n",
       "      <td>alt-rock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 track_id  danceability  energy  speechiness  acousticness  \\\n",
       "0  67hiQr5yocRHh4fheSEzsC         0.358   0.363       0.0288         0.822   \n",
       "1  7aUuoq4oMfLxaLa5GVUDHi         0.590   0.578       0.0528         0.612   \n",
       "\n",
       "   instrumentalness  liveness  valence        artists      album_name  \\\n",
       "0          0.000000    0.3690    0.229  Seiko Matsuda      My Prelude   \n",
       "1          0.000162    0.0837    0.264          KALEO  Way down We Go   \n",
       "\n",
       "       track_name  popularity  duration_ms  explicit  key  loudness  mode  \\\n",
       "0    いくつの夜明けを数えたら          27       303626     False    3    -7.236     1   \n",
       "1  Way down We Go          64       219560     False   10    -5.798     0   \n",
       "\n",
       "     tempo  time_signature track_genre  \n",
       "0  134.649               4      j-idol  \n",
       "1   81.663               4    alt-rock  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "spotify_1 = pd.read_csv('spotify-1.csv')\n",
    "spotify_2 = pd.read_csv('spotify-2.csv')\n",
    "\n",
    "spotify_data = pd.merge(spotify_1, spotify_2, on = 'track_id', how='left')\n",
    "spotify_data.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3512cbd2",
   "metadata": {},
   "source": [
    "#### **Task: Load ``spotify-3.parquet`` and combine it with your result from the previous task.**\n",
    "\n",
    "Note that some columns from the new frame will load with the wrong datatype. To save you time on searching, the columns in question are ``\"popularity\"`` and ``\"explicit\"``. ``pandas`` will, however, not raise an error for this, but will silently raise the dtype for the columns to a superset of both types. Make sure that you change the datatype of the columns to the most _expressive_ one. The resulting DataFrame should have the shape ``(71792, 20)``.\n",
    "\n",
    "If you name your resulting frame ``df``, you can use the assertions in the cell below to check whether your solution worked.\n",
    "\n",
    "**Note:** In this case, it doesn't matter whether you change the datatype of the columns _before_ or _after_ combining the two frames. However, it is better practice to do it beforehand and combine only DataFrames with matching types for all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "65f12a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28717, 20)\n",
      "                 track_id                       artists           album_name  \\\n",
      "0  0Yr1TfeacyGFyDe0aWDla9  Étienne Daho;Italoconnection              Virus X   \n",
      "1  0oOZgg5OXE9ojXfldt4h4P                 Mickie Krause  Finger Im Po Mexiko   \n",
      "\n",
      "                           track_name popularity  duration_ms  explicit  \\\n",
      "0  Virus X - SAGE Rework - radio edit         32       165386         0   \n",
      "1             Laudato Si - DJ Version         25       216093         0   \n",
      "\n",
      "   danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
      "0         0.706   0.766    7    -6.055     0       0.0792       0.00513   \n",
      "1         0.655   0.921    0    -4.415     1       0.0452       0.12500   \n",
      "\n",
      "   instrumentalness  liveness  valence    tempo  time_signature track_genre  \n",
      "0            0.0192     0.199    0.661  101.999               4      french  \n",
      "1            0.0000     0.227    0.842  135.067               4       party  \n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "spotify_3 = pd.read_parquet('spotify-3.parquet')\n",
    "print(spotify_3.shape)\n",
    "print(spotify_3.head(2))\n",
    "\n",
    "# Ensure consistent dtypes\n",
    "spotify_3['popularity'] = spotify_3['popularity'].astype(int)\n",
    "spotify_3['explicit'] = spotify_3['explicit'].astype(bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "701d2d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71792, 20)\n"
     ]
    }
   ],
   "source": [
    "spotify_data = pd.concat([spotify_data, spotify_3], axis = 0, join = 'inner')\n",
    "print(spotify_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c01c11b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert spotify_3[\"popularity\"].dtype == int\n",
    "assert spotify_3[\"explicit\"].dtype == bool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7315482",
   "metadata": {},
   "source": [
    "#### **Task: Filter the columns to only columns which can sensibly contribute to decision-making without overfitting the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9464d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 track_id  danceability  energy  speechiness  acousticness  \\\n",
      "0  67hiQr5yocRHh4fheSEzsC         0.358   0.363       0.0288         0.822   \n",
      "\n",
      "   instrumentalness  liveness  valence  popularity  duration_ms  explicit  \\\n",
      "0               0.0     0.369    0.229          27       303626     False   \n",
      "\n",
      "   key  loudness  mode    tempo  time_signature track_genre  \n",
      "0    3    -7.236     1  134.649               4      j-idol  \n",
      "bool\n",
      "[False  True]\n",
      "                 track_id  danceability  energy  speechiness  acousticness  \\\n",
      "0  67hiQr5yocRHh4fheSEzsC         0.358   0.363       0.0288         0.822   \n",
      "\n",
      "   instrumentalness  liveness  valence  popularity  duration_ms  explicit  \\\n",
      "0               0.0     0.369    0.229          27       303626     False   \n",
      "\n",
      "   key  loudness  mode    tempo  time_signature track_genre  \n",
      "0    3    -7.236     1  134.649               4      j-idol  \n",
      "(71792, 17)\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "#dropping columns with string inputs that would likely skew outcome of id3\n",
    "spotify_data = spotify_data.drop(columns = ['artists', 'album_name', 'track_name', ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0ebdd7",
   "metadata": {},
   "source": [
    "#### **Task: Performing a dataset split – manually**\n",
    "\n",
    "In most cases, you will do just fine with using ``Scikit-Learn``'s ``train_test_split`` (or later: ``PyTorch``'s ``random_split``). However, there are some edge cases where you have to handle splitting yourself, so this task teaches you the basics of how to go about this: _Index Lists_.\n",
    "\n",
    "Essentially, the goal is to list all indices of your data, shuffle that list, and then simply divide it into size-based chunks! In this next cell, write your own function which takes an ``ArrayLike`` object and a list of fractions (i.e. ``float``s) as input and returns a list of ``ArrayLike`` objects of the same lengths as the fraction list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b87e4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def split_array(data : ArrayLike, frac: list[float]):\n",
    "    if not np.isclose(sum(frac), 1):\n",
    "        raise ValueError(f\"frac must sum up to one (roughly)\")\n",
    "    \n",
    "    data_size = len(data)\n",
    "    random_indices = np.random.permutation(len(data))\n",
    "\n",
    "    output_data = []\n",
    "\n",
    "    for i in frac:\n",
    "        size = int(data_size * i)\n",
    "        data_indices = random_indices[:size]\n",
    "        random_indices = random_indices[size:]\n",
    "        output_data.append(data.iloc[data_indices])\n",
    "\n",
    "\n",
    "    return output_data\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab445c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d839c0",
   "metadata": {},
   "source": [
    "### **Learning an Ensemble**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c258cc2a",
   "metadata": {},
   "source": [
    "#### **Task / Baseline: Use ``scikit-learn``'s ``RandomForestClassifier`` to train a random forest of 50 ID3 trees.**\n",
    "\n",
    "Now, load ``spotify-test.csv`` as well and use it with the classifier's ``score`` method. You may need to reorder (``reindex``) the columns in the DataFrame to match the ones of your training frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ddf54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff5fc41",
   "metadata": {},
   "source": [
    "#### **Task: Voting, DIY**\n",
    "\n",
    "Your task is to create your own ensemble of trees, with a twist: Each tree should be trained on a random subset (say, $80\\%$) of the training data, and validated on the rest. You do not need to implement a $k$-fold like system, simply performing a random split each time will suffice. Use the validation scores to create a weighted decision system which takes the validation performance of each individual tree into account. Do not implement the subspace sampling for the trees which is part of the original Random Forest algorithm.\n",
    "\n",
    "You can use the following class skeleton to help get you started!\n",
    "\n",
    "**Note:** Focus on the algorithm, not the performance. Trying to implement this decision algorithm while also trying to maximize performance is very difficult, and is not the goal for this course. It is okay if your implementation is both slower and less powerful than the ``scikit-learn`` base – you are just starting out, after all!\n",
    "\n",
    "**Hint:** To get your bootstrap sample, you can use ``scikit-learn``'s ``resample`` function.\n",
    "\n",
    "**Hint:** Add up all class predictions using the computed weights and return the class with the maximum score!\n",
    "\n",
    "**Hint:** Common errors when working with ``numpy`` arrays arise from using no or incorrect ``dtype`` specifications when creating the arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199aabaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIYForest(ClassifierMixin):\n",
    "    \"\"\"\n",
    "    A DIY Random Forest Class. By using ``ClassifierMixin``, some methods are included automatically, as long as ``fit`` and ``predict`` are implemented. Set the following class attributes in the constructor:\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "\n",
    "    M: np.ndarray\n",
    "        An array of models, in this case DecisionTreeClassifiers. You may also use a list if you aren't comfortable with numpy arrays. Make sure to adjust the type hint in that case.\n",
    "    w: np.ndarray\n",
    "        An array of model weights, which will be filled with validation scores during training. If you use arrays, initialize an array of zeros of the same shape as M in the constructor. If you use lists, you can have this grow organically.\n",
    "    val_size: float\n",
    "        The fraction of the training data to use for validation, i.e. calculating model weights\n",
    "    \"\"\"\n",
    "\n",
    "    M: np.ndarray\n",
    "    w: np.ndarray\n",
    "    val_size: float\n",
    "\n",
    "    def __init__(self, n_trees: int, val_size: float = 0.2, **tree_params) -> None:\n",
    "        \"\"\"\n",
    "        In the constructor, set the class attributes. Initialize tree objects at this point.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_trees: int\n",
    "            How many trees to include in the forest\n",
    "        val_size: float\n",
    "            The fraction of the training data to use for validation\n",
    "        **tree_params: dict\n",
    "            Parameters to pass on to the tree constructor, i.e. ``DecisionTreeClassifier(**tree_params)``.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: your code here\n",
    "\n",
    "    def fit(self, X: ArrayLike, y: ArrayLike) -> DIYForest:\n",
    "        \"\"\"\n",
    "        Fit each tree in the forest using decision attributes ``X`` and target attribute ``y``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: ArrayLike\n",
    "            training examples (only decision attributes)\n",
    "        y: ArrayLike\n",
    "            labels\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: your code here\n",
    "\n",
    "    def predict(self, X: ArrayLike) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        The ensemble makes predictions for the labels of samples ``X``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: ArrayLike\n",
    "            data samples\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Array of shape [X.shape[0],] containing the predictions\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f6f9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23042d5",
   "metadata": {},
   "source": [
    "### **Evaluation**\n",
    "\n",
    "Finally, let's calculate some of the evaluation metrics for the ``scikit-learn`` and our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad8d8b",
   "metadata": {},
   "source": [
    "#### **Accuracy**\n",
    "\n",
    "Accuracy, implemented in ``sklearn.metrics.accuracy_score``, is defined as $$\\operatorname{Accuracy}=\\frac{\\text{TP}+\\text{TN}}{|\\text{TestSet}|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86344e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64b43cb",
   "metadata": {},
   "source": [
    "#### **Precision**\n",
    "\n",
    "Precision, implemented in ``sklearn.metrics.precision_score``, is defined as $$\\operatorname{Precision}=\\frac{\\text{TP}}{\\text{TP}+\\text{FP}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6679e42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594e587a",
   "metadata": {},
   "source": [
    "#### **Recall**\n",
    "\n",
    "Recall, implemented in ``sklearn.metrics.recall_score``, is defined as $$\\operatorname{Recall}=\\frac{\\text{TP}}{\\text{TP}+\\text{FN}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6c4a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637a20be",
   "metadata": {},
   "source": [
    "#### **F1-Score**\n",
    "\n",
    "F1-Score, implemented in ``sklearn.metrics.f1_score``, is defined as $$\\operatorname{F1}=\\frac{2\\times\\text{TP}}{2\\times\\text{TP}+\\text{FP}+\\text{FN}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b289e57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ens-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
